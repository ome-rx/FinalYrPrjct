{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f36e0aea-0dc4-4ba1-93ed-31f92a92b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import CLIPModel\n",
    "import torch.cuda.amp as amp  # For mixed precision training\n",
    "from torch.utils.checkpoint import checkpoint_sequential  # For gradient checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0978772a-06bb-47bc-ad98-7cbc8dff4f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c917b44-4253-4fcc-a175-4cf3bef64299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data preprocessing\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.data.iloc[idx]\n",
    "        image = Image.open(image_path).convert('RGB')  # Convert grayscale images to RGB\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b461e0cf-7453-40cf-bfde-2d337905b39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data directory and emotions\n",
    "data_dir = \"C:\\\\Users\\\\osyed\\\\OneDrive\\\\Desktop\\\\Final Year Project\\\\Combined dataset\"\n",
    "emotions = [\"contempt\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\", \"anger\"]\n",
    "\n",
    "# Create a DataFrame with image paths and labels\n",
    "data = pd.DataFrame([(os.path.join(data_dir, emotion, img), emotions.index(emotion))\n",
    "                     for emotion in emotions\n",
    "                     for img in os.listdir(os.path.join(data_dir, emotion))\n",
    "                     if img.endswith(\".png\") or img.endswith(\".jpg\")],\n",
    "                    columns=[\"image_path\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e7a1137-b65e-43db-9ec8-78359fc9fb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42, stratify=data[\"label\"])\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42, stratify=train_data[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b172b5fe-4e94-4375-a09f-4ed6d7b9595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize all images to 224x224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc06bf1c-146e-41b9-a2ce-0faa36ede624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations to images\n",
    "train_dataset = EmotionDataset(train_data, transform=transform)\n",
    "val_dataset = EmotionDataset(val_data, transform=transform)\n",
    "test_dataset = EmotionDataset(test_data, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e569822b-757b-4b1a-a1d0-41e06fc8f368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "batch_size = 128  # Increase batch size for better GPU utilization\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f01d311d-7d31-4937-a6aa-c213549290cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CLIP model\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4f8edf5-70f2-41be-a671-c1c8c342de3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to the desired device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e06b039-d027-4064-b45b-a48f9f4de871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze CLIP model parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3024c6d4-fe9e-4835-9e40-f32191295621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\osyed\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "C:\\Users\\osyed\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "# Define the EmotionDetector model\n",
    "class EmotionDetector(nn.Module):\n",
    "    def __init__(self, clip_model, num_classes):\n",
    "        super(EmotionDetector, self).__init__()\n",
    "        self.clip_model = clip_model.to(device)  # Move CLIP model to device\n",
    "        \n",
    "        # Get the output dimensions by passing a dummy input through the get_image_features method\n",
    "        dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "        with torch.no_grad():\n",
    "            clip_output = self.clip_model.get_image_features(dummy_input)\n",
    "        self.hidden_size = clip_output.shape[-1]\n",
    "        \n",
    "        self.classifier = nn.Linear(self.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, images):\n",
    "        clip_output = self.clip_model.get_image_features(images)\n",
    "        logits = self.classifier(checkpoint_sequential(clip_output, self.classifier))  # Use gradient checkpointing\n",
    "        return logits\n",
    "\n",
    "num_classes = len(emotions)\n",
    "model = EmotionDetector(model, num_classes).to(device)\n",
    "\n",
    "# Use mixed precision training\n",
    "scaler = amp.GradScaler()\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b80d9a5-b0b8-4d91-9382-9adde2035f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, train_loader, optimizer, scaler, device, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with amp.autocast():  # Mixed precision training\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print(f\"Epoch: {epoch+1}, Train Loss: {train_loss:.4f}\")\n",
    "    return train_loss\n",
    "\n",
    "# Define the validation function\n",
    "def validate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b487e0ef-ddfc-4442-917c-e3d71de726b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, train_loader, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with amp.autocast():  # Mixed precision training\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "\n",
    "    return train_loss / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69cf6ce0-c8a4-4fa3-8dc4-93114f1c2a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchbearer in c:\\users\\osyed\\appdata\\local\\anaconda3\\lib\\site-packages (0.5.5)\n",
      "Requirement already satisfied: torch>=1.0.0 in c:\\users\\osyed\\appdata\\local\\anaconda3\\lib\\site-packages (from torchbearer) (2.2.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\osyed\\appdata\\local\\anaconda3\\lib\\site-packages (from torchbearer) (1.26.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\osyed\\appdata\\local\\anaconda3\\lib\\site-packages (from torchbearer) (4.65.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\osyed\\appdata\\local\\anaconda3\\lib\\site-packages (from torch>=1.0.0->torchbearer) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\osyed\\appdata\\local\\anaconda3\\lib\\site-packages (from torch>=1.0.0->torchbearer) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\osyed\\appdata\\local\\anaconda3\\lib\\site-packages (from torch>=1.0.0->torchbearer) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\osyed\\appdata\\local\\anaconda3\\lib\\site-packages (from torch>=1.0.0->torchbearer) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\osyed\\appdata\\local\\anaconda3\\lib\\site-packages (from torch>=1.0.0->torchbearer) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\osyed\\appdata\\local\\anaconda3\\lib\\site-packages (from torch>=1.0.0->torchbearer) (2023.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\osyed\\appdata\\local\\anaconda3\\lib\\site-packages (from tqdm->torchbearer) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\osyed\\appdata\\local\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.0.0->torchbearer) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\osyed\\appdata\\local\\anaconda3\\lib\\site-packages (from sympy->torch>=1.0.0->torchbearer) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchbearer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a39ab2be-68d5-4ea4-b929-0a51e4de5b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchbearer.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45873e3b-f087-4fce-8a6f-b597b811eabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f03e5a1-55a5-44a2-88ec-2a174cc10d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scaler, device)\n",
    "    val_loss = validate(model, val_loader, device)\n",
    "    scheduler.step(val_loss)\n",
    "    if train_epoch(model, optimizer, scheduler, train_loader, val_loader, epoch, early_stopping):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd869f1-3ce7-47bc-8df1-845b8b838b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "test_acc = 0.0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        test_acc += torch.sum(preds == labels.data)\n",
    "\n",
    "test_acc /= len(test_loader.dataset)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc313a6-f09c-4443-ae24-5b6b9e964fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"emotion_detector2.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8065fe6f-6c54-4135-96a7-c9cc088c3303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d342275f-4530-4740-b1c5-c61c16ffacf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
